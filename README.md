# Flow Log Parser

## Description

This program parses a file containing flow log data and maps each row to a tag based on a lookup table. The lookup table is defined as a CSV file with three columns: `dstport`, `protocol`, and `tag`. Each combination of `dstport` and `protocol` determines what tag is applied to a log entry.

The program supports **version 2** flow logs only, based on the default AWS flow log format. It generates an output file that contains:
1. The count of matches for each tag.
2. The count of matches for each port/protocol combination.

## Assumptions


1. **Flow Log Format**: The program supports only the default AWS flow log format with a fixed structure of 14 fields. Custom log formats or non-standard versions are not supported. Any malformed logs or logs with a different structure will be skipped.
   
2. **Supported Versions**: The program processes only **version 2** flow logs. Entries with other versions are ignored.

3. **Protocols Supported**: The program handles log entries using the **TCP**, **UDP**, and **ICMP** protocols for port-protocol mappings, as defined in the lookup table. The protocol number mapping follows the **IANA (Internet Assigned Numbers Authority)** standards.

4. **Case Insensitivity**: Protocol names are treated in a case-insensitive manner. For example, `TCP` and `tcp` are considered the same.

5. **Input Files**: Both the flow log (`flow_log.txt`) and the lookup table (`lookup.csv`) are assumed to be in plain ASCII text format.

6. **File Sizes**: The program efficiently processes flow log files up to **10 MB** and lookup tables with up to **10,000 mappings**.

7. **Log Entry Status Handling**: Log entries with statuses of `NODATA` or `SKIPDATA` are logged and skipped during processing. These entries do not affect the output.

8. **Performance**: The program processes the flow log entries line by line, making it suitable for handling large log files up to **10 MB** in size without performance issues.

9. **Error Handling**: Any flow log entry that deviates from the expected structure or format (e.g., incorrect number of fields) is skipped, ensuring that only well-formed entries are processed.


## Folder Structure

```
ILLUMIO-FLOWLOG-PARSER/
│
├── data/
│   ├── flow_log.txt      # Input file containing flow log data
│   └── lookup.csv        # Input file containing port, protocol, and tag mappings
│
├── output/
│   └── output.txt        # Output file generated by the program
│
├── src/
│   ├── main.py           # Main flow log parsing script
│   └── __init__.py       # Package marker
│
├── tests/
│   ├── test_flow_log.py  # Unit test script for testing functions
│   └── __init__.py       # Package marker
│
└── README.md             # This README file
```



## How to Run the Program

### Prerequisites

- Python 3.x installed on your system.
- No additional external libraries are required, only built-in Python libraries like `csv`, `os`, `logging`, and `unittest`.

### Instructions

1. **Download the Project**:
   - Clone or download the project folder `ILLUMIO-FLOWLOG-PARSER`.

2. **Prepare Input Files**:
   - Place your `flow_log.txt` and `lookup.csv` files inside the `data/` directory.
   - Example contents of `flow_log.txt` and `lookup.csv` are provided in the `data/` folder.

3. **Run the Program**:
   - Navigate to the `src/` directory and run the `main.py` script:
     ```bash
     python main.py
     ```

4. **Output**:
   - After successful execution, the output will be written to `output/output.txt`.
   - The output file will contain two sections:
     1. **Tag Counts**: The number of flow log entries matched to each tag.
     2. **Port/Protocol Combination Counts**: The number of flow log entries matched to each (port, protocol) combination.

### Example of Output

```Tag Counts:
Tag,Count
sv_P1,2
email,3
Untagged,9

Port/Protocol Combination Counts:
Port,Protocol,Count
25,tcp,1
110,tcp,1
993,tcp,1
143,tcp,1
1024,tcp,1
80,tcp,1
```



## Testing

### Unit Tests

Unit tests are provided in the `tests/test_flow_log.py` file to validate the following:
- Parsing of the lookup table.
- Processing of valid and malformed flow log lines.
- Correct handling of "NODATA" and "SKIPDATA" log entries.
- Generation of output with accurate tag counts and port/protocol counts.

### Running the Tests

To run the tests, navigate to the root project directory and execute the following command:
```bash
python -m unittest discover -s tests
```

This will automatically discover and run all unit tests in the `tests/` directory.

## Logging

The program uses Python's built-in `logging` module to record important events, warnings, and errors:

- **Warnings** are logged for malformed flow log lines or unknown protocols.
- **Info** logs are created for skipped `NODATA` or `SKIPDATA` records.
- **Errors** are logged for missing or corrupted input files.

All logs are printed to the console for easy monitoring during execution.

## Code Analysis

### Design and Efficiency

1. **Simplicity**: The program processes flow logs line-by-line, which minimizes memory usage and makes it efficient for handling files up to **10 MB**. It uses only built-in Python libraries, ensuring ease of use without external dependencies.

2. **Protocol Mapping**: Protocol numbers are mapped using standard IANA protocol numbers (e.g., `6` for TCP, `17` for UDP, `1` for ICMP), ensuring compatibility with standard networking conventions.

3. **Error Handling**: The program gracefully handles malformed lines, unsupported flow log versions, and entries with `NODATA` or `SKIPDATA`, logging useful warnings without crashing.

4. **Modular Structure**: Core functions (parsing, processing, and writing output) are modular, improving maintainability, readability, and ease of testing.

5. **Performance**: The lookup table is stored in a dictionary for efficient constant-time lookups (`O(1)`), and the program processes logs line-by-line for better memory performance.

### Potential Improvements

1. **Custom Protocol Mappings**: Allow users to provide custom protocol mappings via a configuration file.
   
2. **Parallel Processing**: Introduce parallel processing for very large files to improve performance.

3. **Additional Formats**: Extend the program to support custom log formats beyond the AWS version 2 flow logs.

## Conclusion

The program is designed to be efficient, easy to use, and maintainable. It handles large log files effectively, while providing flexibility for future enhancements.

